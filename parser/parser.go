// Package parser is generated by gogll. Do not edit.
package parser

import (
	"bytes"
	"fmt"
	"sort"
	"strings"

	"github.com/goccmack/gogll/lexer"
	"github.com/goccmack/gogll/parser/bsr"
	"github.com/goccmack/gogll/parser/slot"
	"github.com/goccmack/gogll/parser/symbols"
	"github.com/goccmack/gogll/token"
)

type parser struct {
	cI int

	R *descriptors
	U *descriptors

	popped   map[poppedNode]bool
	crf      map[clusterNode][]*crfNode
	crfNodes map[crfNode]*crfNode

	lex         *lexer.Lexer
	parseErrors []*Error

	bsrSet *bsr.Set
}

func newParser(l *lexer.Lexer) *parser {
	return &parser{
		cI:     0,
		lex:    l,
		R:      &descriptors{},
		U:      &descriptors{},
		popped: make(map[poppedNode]bool),
		crf: map[clusterNode][]*crfNode{
			{symbols.NT_GoGLL, 0}: {},
		},
		crfNodes:    map[crfNode]*crfNode{},
		bsrSet:      bsr.New(symbols.NT_GoGLL, l),
		parseErrors: nil,
	}
}

// Parse returns the BSR set containing the parse forest.
// If the parse was successfull []*Error is nil
func Parse(l *lexer.Lexer) (*bsr.Set, []*Error) {
	return newParser(l).parse()
}

func (p *parser) parse() (*bsr.Set, []*Error) {
	var L slot.Label
	m, cU := len(p.lex.Tokens)-1, 0
	p.ntAdd(symbols.NT_GoGLL, 0)
	// p.DumpDescriptors()
	for !p.R.empty() {
		L, cU, p.cI = p.R.remove()

		// fmt.Println()
		// fmt.Printf("L:%s, cI:%d, I[p.cI]:%s, cU:%d\n", L, p.cI, p.lex.Tokens[p.cI], cU)
		// p.DumpDescriptors()

		switch L {
		case slot.GoGLL0R0: // GoGLL : ∙Package Rules

			p.call(slot.GoGLL0R1, cU, p.cI)
		case slot.GoGLL0R1: // GoGLL : Package ∙Rules

			if !p.testSelect(slot.GoGLL0R1) {
				p.parseError(slot.GoGLL0R1, p.cI, first[slot.GoGLL0R1])
				break
			}

			p.call(slot.GoGLL0R2, cU, p.cI)
		case slot.GoGLL0R2: // GoGLL : Package Rules ∙

			if p.follow(symbols.NT_GoGLL) {
				p.rtn(symbols.NT_GoGLL, cU, p.cI)
			} else {
				p.parseError(slot.GoGLL0R0, p.cI, followSets[symbols.NT_GoGLL])
			}
		case slot.LexAlternates0R0: // LexAlternates : ∙RegExp

			p.call(slot.LexAlternates0R1, cU, p.cI)
		case slot.LexAlternates0R1: // LexAlternates : RegExp ∙

			if p.follow(symbols.NT_LexAlternates) {
				p.rtn(symbols.NT_LexAlternates, cU, p.cI)
			} else {
				p.parseError(slot.LexAlternates0R0, p.cI, followSets[symbols.NT_LexAlternates])
			}
		case slot.LexAlternates1R0: // LexAlternates : ∙RegExp | LexAlternates

			p.call(slot.LexAlternates1R1, cU, p.cI)
		case slot.LexAlternates1R1: // LexAlternates : RegExp ∙| LexAlternates

			if !p.testSelect(slot.LexAlternates1R1) {
				p.parseError(slot.LexAlternates1R1, p.cI, first[slot.LexAlternates1R1])
				break
			}

			p.bsrSet.Add(slot.LexAlternates1R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexAlternates1R2) {
				p.parseError(slot.LexAlternates1R2, p.cI, first[slot.LexAlternates1R2])
				break
			}

			p.call(slot.LexAlternates1R3, cU, p.cI)
		case slot.LexAlternates1R3: // LexAlternates : RegExp | LexAlternates ∙

			if p.follow(symbols.NT_LexAlternates) {
				p.rtn(symbols.NT_LexAlternates, cU, p.cI)
			} else {
				p.parseError(slot.LexAlternates1R0, p.cI, followSets[symbols.NT_LexAlternates])
			}
		case slot.LexBracket0R0: // LexBracket : ∙LexGroup

			p.call(slot.LexBracket0R1, cU, p.cI)
		case slot.LexBracket0R1: // LexBracket : LexGroup ∙

			if p.follow(symbols.NT_LexBracket) {
				p.rtn(symbols.NT_LexBracket, cU, p.cI)
			} else {
				p.parseError(slot.LexBracket0R0, p.cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexBracket1R0: // LexBracket : ∙LexOptional

			p.call(slot.LexBracket1R1, cU, p.cI)
		case slot.LexBracket1R1: // LexBracket : LexOptional ∙

			if p.follow(symbols.NT_LexBracket) {
				p.rtn(symbols.NT_LexBracket, cU, p.cI)
			} else {
				p.parseError(slot.LexBracket1R0, p.cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexBracket2R0: // LexBracket : ∙LexZeroOrMore

			p.call(slot.LexBracket2R1, cU, p.cI)
		case slot.LexBracket2R1: // LexBracket : LexZeroOrMore ∙

			if p.follow(symbols.NT_LexBracket) {
				p.rtn(symbols.NT_LexBracket, cU, p.cI)
			} else {
				p.parseError(slot.LexBracket2R0, p.cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexBracket3R0: // LexBracket : ∙LexOneOrMore

			p.call(slot.LexBracket3R1, cU, p.cI)
		case slot.LexBracket3R1: // LexBracket : LexOneOrMore ∙

			if p.follow(symbols.NT_LexBracket) {
				p.rtn(symbols.NT_LexBracket, cU, p.cI)
			} else {
				p.parseError(slot.LexBracket3R0, p.cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexGroup0R0: // LexGroup : ∙( LexAlternates )

			p.bsrSet.Add(slot.LexGroup0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexGroup0R1) {
				p.parseError(slot.LexGroup0R1, p.cI, first[slot.LexGroup0R1])
				break
			}

			p.call(slot.LexGroup0R2, cU, p.cI)
		case slot.LexGroup0R2: // LexGroup : ( LexAlternates ∙)

			if !p.testSelect(slot.LexGroup0R2) {
				p.parseError(slot.LexGroup0R2, p.cI, first[slot.LexGroup0R2])
				break
			}

			p.bsrSet.Add(slot.LexGroup0R3, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexGroup) {
				p.rtn(symbols.NT_LexGroup, cU, p.cI)
			} else {
				p.parseError(slot.LexGroup0R0, p.cI, followSets[symbols.NT_LexGroup])
			}
		case slot.LexOneOrMore0R0: // LexOneOrMore : ∙< LexAlternates >

			p.bsrSet.Add(slot.LexOneOrMore0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexOneOrMore0R1) {
				p.parseError(slot.LexOneOrMore0R1, p.cI, first[slot.LexOneOrMore0R1])
				break
			}

			p.call(slot.LexOneOrMore0R2, cU, p.cI)
		case slot.LexOneOrMore0R2: // LexOneOrMore : < LexAlternates ∙>

			if !p.testSelect(slot.LexOneOrMore0R2) {
				p.parseError(slot.LexOneOrMore0R2, p.cI, first[slot.LexOneOrMore0R2])
				break
			}

			p.bsrSet.Add(slot.LexOneOrMore0R3, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexOneOrMore) {
				p.rtn(symbols.NT_LexOneOrMore, cU, p.cI)
			} else {
				p.parseError(slot.LexOneOrMore0R0, p.cI, followSets[symbols.NT_LexOneOrMore])
			}
		case slot.LexOptional0R0: // LexOptional : ∙[ LexAlternates ]

			p.bsrSet.Add(slot.LexOptional0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexOptional0R1) {
				p.parseError(slot.LexOptional0R1, p.cI, first[slot.LexOptional0R1])
				break
			}

			p.call(slot.LexOptional0R2, cU, p.cI)
		case slot.LexOptional0R2: // LexOptional : [ LexAlternates ∙]

			if !p.testSelect(slot.LexOptional0R2) {
				p.parseError(slot.LexOptional0R2, p.cI, first[slot.LexOptional0R2])
				break
			}

			p.bsrSet.Add(slot.LexOptional0R3, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexOptional) {
				p.rtn(symbols.NT_LexOptional, cU, p.cI)
			} else {
				p.parseError(slot.LexOptional0R0, p.cI, followSets[symbols.NT_LexOptional])
			}
		case slot.LexRule0R0: // LexRule : ∙tokid : RegExp ;

			p.bsrSet.Add(slot.LexRule0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexRule0R1) {
				p.parseError(slot.LexRule0R1, p.cI, first[slot.LexRule0R1])
				break
			}

			p.bsrSet.Add(slot.LexRule0R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexRule0R2) {
				p.parseError(slot.LexRule0R2, p.cI, first[slot.LexRule0R2])
				break
			}

			p.call(slot.LexRule0R3, cU, p.cI)
		case slot.LexRule0R3: // LexRule : tokid : RegExp ∙;

			if !p.testSelect(slot.LexRule0R3) {
				p.parseError(slot.LexRule0R3, p.cI, first[slot.LexRule0R3])
				break
			}

			p.bsrSet.Add(slot.LexRule0R4, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexRule) {
				p.rtn(symbols.NT_LexRule, cU, p.cI)
			} else {
				p.parseError(slot.LexRule0R0, p.cI, followSets[symbols.NT_LexRule])
			}
		case slot.LexSymbol0R0: // LexSymbol : ∙.

			p.bsrSet.Add(slot.LexSymbol0R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexSymbol) {
				p.rtn(symbols.NT_LexSymbol, cU, p.cI)
			} else {
				p.parseError(slot.LexSymbol0R0, p.cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol1R0: // LexSymbol : ∙any string_lit

			p.bsrSet.Add(slot.LexSymbol1R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexSymbol1R1) {
				p.parseError(slot.LexSymbol1R1, p.cI, first[slot.LexSymbol1R1])
				break
			}

			p.bsrSet.Add(slot.LexSymbol1R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexSymbol) {
				p.rtn(symbols.NT_LexSymbol, cU, p.cI)
			} else {
				p.parseError(slot.LexSymbol1R0, p.cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol2R0: // LexSymbol : ∙char_lit

			p.bsrSet.Add(slot.LexSymbol2R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexSymbol) {
				p.rtn(symbols.NT_LexSymbol, cU, p.cI)
			} else {
				p.parseError(slot.LexSymbol2R0, p.cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol3R0: // LexSymbol : ∙LexBracket

			p.call(slot.LexSymbol3R1, cU, p.cI)
		case slot.LexSymbol3R1: // LexSymbol : LexBracket ∙

			if p.follow(symbols.NT_LexSymbol) {
				p.rtn(symbols.NT_LexSymbol, cU, p.cI)
			} else {
				p.parseError(slot.LexSymbol3R0, p.cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol4R0: // LexSymbol : ∙not string_lit

			p.bsrSet.Add(slot.LexSymbol4R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexSymbol4R1) {
				p.parseError(slot.LexSymbol4R1, p.cI, first[slot.LexSymbol4R1])
				break
			}

			p.bsrSet.Add(slot.LexSymbol4R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexSymbol) {
				p.rtn(symbols.NT_LexSymbol, cU, p.cI)
			} else {
				p.parseError(slot.LexSymbol4R0, p.cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol5R0: // LexSymbol : ∙UnicodeClass

			p.call(slot.LexSymbol5R1, cU, p.cI)
		case slot.LexSymbol5R1: // LexSymbol : UnicodeClass ∙

			if p.follow(symbols.NT_LexSymbol) {
				p.rtn(symbols.NT_LexSymbol, cU, p.cI)
			} else {
				p.parseError(slot.LexSymbol5R0, p.cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexZeroOrMore0R0: // LexZeroOrMore : ∙{ LexAlternates }

			p.bsrSet.Add(slot.LexZeroOrMore0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.LexZeroOrMore0R1) {
				p.parseError(slot.LexZeroOrMore0R1, p.cI, first[slot.LexZeroOrMore0R1])
				break
			}

			p.call(slot.LexZeroOrMore0R2, cU, p.cI)
		case slot.LexZeroOrMore0R2: // LexZeroOrMore : { LexAlternates ∙}

			if !p.testSelect(slot.LexZeroOrMore0R2) {
				p.parseError(slot.LexZeroOrMore0R2, p.cI, first[slot.LexZeroOrMore0R2])
				break
			}

			p.bsrSet.Add(slot.LexZeroOrMore0R3, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_LexZeroOrMore) {
				p.rtn(symbols.NT_LexZeroOrMore, cU, p.cI)
			} else {
				p.parseError(slot.LexZeroOrMore0R0, p.cI, followSets[symbols.NT_LexZeroOrMore])
			}
		case slot.Package0R0: // Package : ∙package string_lit

			p.bsrSet.Add(slot.Package0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.Package0R1) {
				p.parseError(slot.Package0R1, p.cI, first[slot.Package0R1])
				break
			}

			p.bsrSet.Add(slot.Package0R2, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_Package) {
				p.rtn(symbols.NT_Package, cU, p.cI)
			} else {
				p.parseError(slot.Package0R0, p.cI, followSets[symbols.NT_Package])
			}
		case slot.RegExp0R0: // RegExp : ∙LexSymbol

			p.call(slot.RegExp0R1, cU, p.cI)
		case slot.RegExp0R1: // RegExp : LexSymbol ∙

			if p.follow(symbols.NT_RegExp) {
				p.rtn(symbols.NT_RegExp, cU, p.cI)
			} else {
				p.parseError(slot.RegExp0R0, p.cI, followSets[symbols.NT_RegExp])
			}
		case slot.RegExp1R0: // RegExp : ∙LexSymbol RegExp

			p.call(slot.RegExp1R1, cU, p.cI)
		case slot.RegExp1R1: // RegExp : LexSymbol ∙RegExp

			if !p.testSelect(slot.RegExp1R1) {
				p.parseError(slot.RegExp1R1, p.cI, first[slot.RegExp1R1])
				break
			}

			p.call(slot.RegExp1R2, cU, p.cI)
		case slot.RegExp1R2: // RegExp : LexSymbol RegExp ∙

			if p.follow(symbols.NT_RegExp) {
				p.rtn(symbols.NT_RegExp, cU, p.cI)
			} else {
				p.parseError(slot.RegExp1R0, p.cI, followSets[symbols.NT_RegExp])
			}
		case slot.Rule0R0: // Rule : ∙LexRule

			p.call(slot.Rule0R1, cU, p.cI)
		case slot.Rule0R1: // Rule : LexRule ∙

			if p.follow(symbols.NT_Rule) {
				p.rtn(symbols.NT_Rule, cU, p.cI)
			} else {
				p.parseError(slot.Rule0R0, p.cI, followSets[symbols.NT_Rule])
			}
		case slot.Rule1R0: // Rule : ∙SyntaxRule

			p.call(slot.Rule1R1, cU, p.cI)
		case slot.Rule1R1: // Rule : SyntaxRule ∙

			if p.follow(symbols.NT_Rule) {
				p.rtn(symbols.NT_Rule, cU, p.cI)
			} else {
				p.parseError(slot.Rule1R0, p.cI, followSets[symbols.NT_Rule])
			}
		case slot.Rules0R0: // Rules : ∙Rule

			p.call(slot.Rules0R1, cU, p.cI)
		case slot.Rules0R1: // Rules : Rule ∙

			if p.follow(symbols.NT_Rules) {
				p.rtn(symbols.NT_Rules, cU, p.cI)
			} else {
				p.parseError(slot.Rules0R0, p.cI, followSets[symbols.NT_Rules])
			}
		case slot.Rules1R0: // Rules : ∙Rule Rules

			p.call(slot.Rules1R1, cU, p.cI)
		case slot.Rules1R1: // Rules : Rule ∙Rules

			if !p.testSelect(slot.Rules1R1) {
				p.parseError(slot.Rules1R1, p.cI, first[slot.Rules1R1])
				break
			}

			p.call(slot.Rules1R2, cU, p.cI)
		case slot.Rules1R2: // Rules : Rule Rules ∙

			if p.follow(symbols.NT_Rules) {
				p.rtn(symbols.NT_Rules, cU, p.cI)
			} else {
				p.parseError(slot.Rules1R0, p.cI, followSets[symbols.NT_Rules])
			}
		case slot.SyntaxAlternate0R0: // SyntaxAlternate : ∙SyntaxSymbols

			p.call(slot.SyntaxAlternate0R1, cU, p.cI)
		case slot.SyntaxAlternate0R1: // SyntaxAlternate : SyntaxSymbols ∙

			if p.follow(symbols.NT_SyntaxAlternate) {
				p.rtn(symbols.NT_SyntaxAlternate, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxAlternate0R0, p.cI, followSets[symbols.NT_SyntaxAlternate])
			}
		case slot.SyntaxAlternate1R0: // SyntaxAlternate : ∙empty

			p.bsrSet.Add(slot.SyntaxAlternate1R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_SyntaxAlternate) {
				p.rtn(symbols.NT_SyntaxAlternate, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxAlternate1R0, p.cI, followSets[symbols.NT_SyntaxAlternate])
			}
		case slot.SyntaxAlternates0R0: // SyntaxAlternates : ∙SyntaxAlternate

			p.call(slot.SyntaxAlternates0R1, cU, p.cI)
		case slot.SyntaxAlternates0R1: // SyntaxAlternates : SyntaxAlternate ∙

			if p.follow(symbols.NT_SyntaxAlternates) {
				p.rtn(symbols.NT_SyntaxAlternates, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxAlternates0R0, p.cI, followSets[symbols.NT_SyntaxAlternates])
			}
		case slot.SyntaxAlternates1R0: // SyntaxAlternates : ∙SyntaxAlternate | SyntaxAlternates

			p.call(slot.SyntaxAlternates1R1, cU, p.cI)
		case slot.SyntaxAlternates1R1: // SyntaxAlternates : SyntaxAlternate ∙| SyntaxAlternates

			if !p.testSelect(slot.SyntaxAlternates1R1) {
				p.parseError(slot.SyntaxAlternates1R1, p.cI, first[slot.SyntaxAlternates1R1])
				break
			}

			p.bsrSet.Add(slot.SyntaxAlternates1R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.SyntaxAlternates1R2) {
				p.parseError(slot.SyntaxAlternates1R2, p.cI, first[slot.SyntaxAlternates1R2])
				break
			}

			p.call(slot.SyntaxAlternates1R3, cU, p.cI)
		case slot.SyntaxAlternates1R3: // SyntaxAlternates : SyntaxAlternate | SyntaxAlternates ∙

			if p.follow(symbols.NT_SyntaxAlternates) {
				p.rtn(symbols.NT_SyntaxAlternates, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxAlternates1R0, p.cI, followSets[symbols.NT_SyntaxAlternates])
			}
		case slot.SyntaxRule0R0: // SyntaxRule : ∙nt : SyntaxAlternates ;

			p.bsrSet.Add(slot.SyntaxRule0R1, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.SyntaxRule0R1) {
				p.parseError(slot.SyntaxRule0R1, p.cI, first[slot.SyntaxRule0R1])
				break
			}

			p.bsrSet.Add(slot.SyntaxRule0R2, cU, p.cI, p.cI+1)
			p.cI++
			if !p.testSelect(slot.SyntaxRule0R2) {
				p.parseError(slot.SyntaxRule0R2, p.cI, first[slot.SyntaxRule0R2])
				break
			}

			p.call(slot.SyntaxRule0R3, cU, p.cI)
		case slot.SyntaxRule0R3: // SyntaxRule : nt : SyntaxAlternates ∙;

			if !p.testSelect(slot.SyntaxRule0R3) {
				p.parseError(slot.SyntaxRule0R3, p.cI, first[slot.SyntaxRule0R3])
				break
			}

			p.bsrSet.Add(slot.SyntaxRule0R4, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_SyntaxRule) {
				p.rtn(symbols.NT_SyntaxRule, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxRule0R0, p.cI, followSets[symbols.NT_SyntaxRule])
			}
		case slot.SyntaxSymbol0R0: // SyntaxSymbol : ∙nt

			p.bsrSet.Add(slot.SyntaxSymbol0R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_SyntaxSymbol) {
				p.rtn(symbols.NT_SyntaxSymbol, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxSymbol0R0, p.cI, followSets[symbols.NT_SyntaxSymbol])
			}
		case slot.SyntaxSymbol1R0: // SyntaxSymbol : ∙tokid

			p.bsrSet.Add(slot.SyntaxSymbol1R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_SyntaxSymbol) {
				p.rtn(symbols.NT_SyntaxSymbol, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxSymbol1R0, p.cI, followSets[symbols.NT_SyntaxSymbol])
			}
		case slot.SyntaxSymbol2R0: // SyntaxSymbol : ∙string_lit

			p.bsrSet.Add(slot.SyntaxSymbol2R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_SyntaxSymbol) {
				p.rtn(symbols.NT_SyntaxSymbol, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxSymbol2R0, p.cI, followSets[symbols.NT_SyntaxSymbol])
			}
		case slot.SyntaxSymbols0R0: // SyntaxSymbols : ∙SyntaxSymbol

			p.call(slot.SyntaxSymbols0R1, cU, p.cI)
		case slot.SyntaxSymbols0R1: // SyntaxSymbols : SyntaxSymbol ∙

			if p.follow(symbols.NT_SyntaxSymbols) {
				p.rtn(symbols.NT_SyntaxSymbols, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxSymbols0R0, p.cI, followSets[symbols.NT_SyntaxSymbols])
			}
		case slot.SyntaxSymbols1R0: // SyntaxSymbols : ∙SyntaxSymbol SyntaxSymbols

			p.call(slot.SyntaxSymbols1R1, cU, p.cI)
		case slot.SyntaxSymbols1R1: // SyntaxSymbols : SyntaxSymbol ∙SyntaxSymbols

			if !p.testSelect(slot.SyntaxSymbols1R1) {
				p.parseError(slot.SyntaxSymbols1R1, p.cI, first[slot.SyntaxSymbols1R1])
				break
			}

			p.call(slot.SyntaxSymbols1R2, cU, p.cI)
		case slot.SyntaxSymbols1R2: // SyntaxSymbols : SyntaxSymbol SyntaxSymbols ∙

			if p.follow(symbols.NT_SyntaxSymbols) {
				p.rtn(symbols.NT_SyntaxSymbols, cU, p.cI)
			} else {
				p.parseError(slot.SyntaxSymbols1R0, p.cI, followSets[symbols.NT_SyntaxSymbols])
			}
		case slot.UnicodeClass0R0: // UnicodeClass : ∙letter

			p.bsrSet.Add(slot.UnicodeClass0R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_UnicodeClass) {
				p.rtn(symbols.NT_UnicodeClass, cU, p.cI)
			} else {
				p.parseError(slot.UnicodeClass0R0, p.cI, followSets[symbols.NT_UnicodeClass])
			}
		case slot.UnicodeClass1R0: // UnicodeClass : ∙upcase

			p.bsrSet.Add(slot.UnicodeClass1R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_UnicodeClass) {
				p.rtn(symbols.NT_UnicodeClass, cU, p.cI)
			} else {
				p.parseError(slot.UnicodeClass1R0, p.cI, followSets[symbols.NT_UnicodeClass])
			}
		case slot.UnicodeClass2R0: // UnicodeClass : ∙lowcase

			p.bsrSet.Add(slot.UnicodeClass2R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_UnicodeClass) {
				p.rtn(symbols.NT_UnicodeClass, cU, p.cI)
			} else {
				p.parseError(slot.UnicodeClass2R0, p.cI, followSets[symbols.NT_UnicodeClass])
			}
		case slot.UnicodeClass3R0: // UnicodeClass : ∙number

			p.bsrSet.Add(slot.UnicodeClass3R1, cU, p.cI, p.cI+1)
			p.cI++
			if p.follow(symbols.NT_UnicodeClass) {
				p.rtn(symbols.NT_UnicodeClass, cU, p.cI)
			} else {
				p.parseError(slot.UnicodeClass3R0, p.cI, followSets[symbols.NT_UnicodeClass])
			}

		default:
			panic("This must not happen")
		}
	}
	if !p.bsrSet.Contain(symbols.NT_GoGLL, 0, m) {
		p.sortParseErrors()
		return nil, p.parseErrors
	}
	return p.bsrSet, nil
}

func (p *parser) ntAdd(nt symbols.NT, j int) {
	// fmt.Printf("p.ntAdd(%s, %d)\n", nt, j)
	failed := true
	expected := map[token.Type]string{}
	for _, l := range slot.GetAlternates(nt) {
		if p.testSelect(l) {
			p.dscAdd(l, j, j)
			failed = false
		} else {
			for k, v := range first[l] {
				expected[k] = v
			}
		}
	}
	if failed {
		for _, l := range slot.GetAlternates(nt) {
			p.parseError(l, j, expected)
		}
	}
}

/*** Call Return Forest ***/

type poppedNode struct {
	X    symbols.NT
	k, j int
}

type clusterNode struct {
	X symbols.NT
	k int
}

type crfNode struct {
	L slot.Label
	i int
}

/*
suppose that L is Y ::=αX ·β
if there is no CRF node labelled (L,i)
	create one let u be the CRF node labelled (L,i)
if there is no CRF node labelled (X, j) {
	create a CRF node v labelled (X, j)
	create an edge from v to u
	ntAdd(X, j)
} else {
	let v be the CRF node labelled (X, j)
	if there is not an edge from v to u {
		create an edge from v to u
		for all ((X, j,h)∈P) {
			dscAdd(L, i, h);
			bsrAdd(L, i, j, h)
		}
	}
}
*/
func (p *parser) call(L slot.Label, i, j int) {
	// fmt.Printf("p.call(%s,%d,%d)\n", L,i,j)
	u, exist := p.crfNodes[crfNode{L, i}]
	// fmt.Printf("  u exist=%t\n", exist)
	if !exist {
		u = &crfNode{L, i}
		p.crfNodes[*u] = u
	}
	X := L.Symbols()[L.Pos()-1].(symbols.NT)
	ndV := clusterNode{X, j}
	v, exist := p.crf[ndV]
	if !exist {
		// fmt.Println("  v !exist")
		p.crf[ndV] = []*crfNode{u}
		p.ntAdd(X, j)
	} else {
		// fmt.Println("  v exist")
		if !existEdge(v, u) {
			// fmt.Printf("  !existEdge(%v)\n", u)
			p.crf[ndV] = append(v, u)
			// fmt.Printf("|popped|=%d\n", len(popped))
			for pnd := range p.popped {
				if pnd.X == X && pnd.k == j {
					p.dscAdd(L, i, pnd.j)
					p.bsrSet.Add(L, i, j, pnd.j)
				}
			}
		}
	}
}

func existEdge(nds []*crfNode, nd *crfNode) bool {
	for _, nd1 := range nds {
		if nd1 == nd {
			return true
		}
	}
	return false
}

func (p *parser) rtn(X symbols.NT, k, j int) {
	// fmt.Printf("p.rtn(%s,%d,%d)\n", X,k,j)
	pn := poppedNode{X, k, j}
	if _, exist := p.popped[pn]; !exist {
		p.popped[pn] = true
		for _, nd := range p.crf[clusterNode{X, k}] {
			p.dscAdd(nd.L, nd.i, j)
			p.bsrSet.Add(nd.L, nd.i, k, j)
		}
	}
}

// func CRFString() string {
// 	buf := new(bytes.Buffer)
// 	buf.WriteString("CRF: {")
// 	for cn, nds := range crf{
// 		for _, nd := range nds {
// 			fmt.Fprintf(buf, "%s->%s, ", cn, nd)
// 		}
// 	}
// 	buf.WriteString("}")
// 	return buf.String()
// }

func (cn clusterNode) String() string {
	return fmt.Sprintf("(%s,%d)", cn.X, cn.k)
}

func (n crfNode) String() string {
	return fmt.Sprintf("(%s,%d)", n.L.String(), n.i)
}

// func PoppedString() string {
// 	buf := new(bytes.Buffer)
// 	buf.WriteString("Popped: {")
// 	for p, _ := range popped {
// 		fmt.Fprintf(buf, "(%s,%d,%d) ", p.X, p.k, p.j)
// 	}
// 	buf.WriteString("}")
// 	return buf.String()
// }

/*** descriptors ***/

type descriptors struct {
	set []*descriptor
}

func (ds *descriptors) contain(d *descriptor) bool {
	for _, d1 := range ds.set {
		if d1 == d {
			return true
		}
	}
	return false
}

func (ds *descriptors) empty() bool {
	return len(ds.set) == 0
}

func (ds *descriptors) String() string {
	buf := new(bytes.Buffer)
	buf.WriteString("{")
	for i, d := range ds.set {
		if i > 0 {
			buf.WriteString("; ")
		}
		fmt.Fprintf(buf, "%s", d)
	}
	buf.WriteString("}")
	return buf.String()
}

type descriptor struct {
	L slot.Label
	k int
	i int
}

func (d *descriptor) String() string {
	return fmt.Sprintf("%s,%d,%d", d.L, d.k, d.i)
}

func (p *parser) dscAdd(L slot.Label, k, i int) {
	// fmt.Printf("p.dscAdd(%s,%d,%d)\n", L, k, i)
	d := &descriptor{L, k, i}
	if !p.U.contain(d) {
		p.R.set = append(p.R.set, d)
		p.U.set = append(p.U.set, d)
	}
}

func (ds *descriptors) remove() (L slot.Label, k, i int) {
	d := ds.set[len(ds.set)-1]
	ds.set = ds.set[:len(ds.set)-1]
	// fmt.Printf("remove: %s,%d,%d\n", d.L, d.k, d.i)
	return d.L, d.k, d.i
}

func (p *parser) DumpDescriptors() {
	p.DumpR()
	p.DumpU()
}

func (p *parser) DumpR() {
	fmt.Println("R:")
	for _, d := range p.R.set {
		fmt.Printf(" %s\n", d)
	}
}

func (p *parser) DumpU() {
	fmt.Println("U:")
	for _, d := range p.U.set {
		fmt.Printf(" %s\n", d)
	}
}

/*** TestSelect ***/

func (p *parser) follow(nt symbols.NT) bool {
	_, exist := followSets[nt][p.lex.Tokens[p.cI].Type()]
	return exist
}

func (p *parser) testSelect(l slot.Label) bool {
	_, exist := first[l][p.lex.Tokens[p.cI].Type()]
	// fmt.Printf("testSelect(%s) = %t\n", l, exist)
	return exist
}

var first = []map[token.Type]string{
	// GoGLL : ∙Package Rules
	{
		token.Type21: "package",
	},
	// GoGLL : Package ∙Rules
	{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// GoGLL : Package Rules ∙
	{
		token.EOF: "EOF",
	},
	// LexAlternates : ∙RegExp
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexAlternates : RegExp ∙
	{
		token.Type2:  ")",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type27: "}",
	},
	// LexAlternates : ∙RegExp | LexAlternates
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexAlternates : RegExp ∙| LexAlternates
	{
		token.Type26: "|",
	},
	// LexAlternates : RegExp | ∙LexAlternates
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexAlternates : RegExp | LexAlternates ∙
	{
		token.Type2:  ")",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type27: "}",
	},
	// LexBracket : ∙LexGroup
	{
		token.Type1: "(",
	},
	// LexBracket : LexGroup ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexBracket : ∙LexOptional
	{
		token.Type8: "[",
	},
	// LexBracket : LexOptional ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexBracket : ∙LexZeroOrMore
	{
		token.Type25: "{",
	},
	// LexBracket : LexZeroOrMore ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexBracket : ∙LexOneOrMore
	{
		token.Type6: "<",
	},
	// LexBracket : LexOneOrMore ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexGroup : ∙( LexAlternates )
	{
		token.Type1: "(",
	},
	// LexGroup : ( ∙LexAlternates )
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexGroup : ( LexAlternates ∙)
	{
		token.Type2: ")",
	},
	// LexGroup : ( LexAlternates ) ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOneOrMore : ∙< LexAlternates >
	{
		token.Type6: "<",
	},
	// LexOneOrMore : < ∙LexAlternates >
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexOneOrMore : < LexAlternates ∙>
	{
		token.Type7: ">",
	},
	// LexOneOrMore : < LexAlternates > ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOptional : ∙[ LexAlternates ]
	{
		token.Type8: "[",
	},
	// LexOptional : [ ∙LexAlternates ]
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexOptional : [ LexAlternates ∙]
	{
		token.Type11: "]",
	},
	// LexOptional : [ LexAlternates ] ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexRule : ∙tokid : RegExp ;
	{
		token.Type23: "tokid",
	},
	// LexRule : tokid ∙: RegExp ;
	{
		token.Type4: ":",
	},
	// LexRule : tokid : ∙RegExp ;
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexRule : tokid : RegExp ∙;
	{
		token.Type5: ";",
	},
	// LexRule : tokid : RegExp ; ∙
	{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// LexSymbol : ∙.
	{
		token.Type3: ".",
	},
	// LexSymbol : . ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙any string_lit
	{
		token.Type13: "any",
	},
	// LexSymbol : any ∙string_lit
	{
		token.Type22: "string_lit",
	},
	// LexSymbol : any string_lit ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙char_lit
	{
		token.Type14: "char_lit",
	},
	// LexSymbol : char_lit ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙LexBracket
	{
		token.Type1:  "(",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type25: "{",
	},
	// LexSymbol : LexBracket ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙not string_lit
	{
		token.Type18: "not",
	},
	// LexSymbol : not ∙string_lit
	{
		token.Type22: "string_lit",
	},
	// LexSymbol : not string_lit ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙UnicodeClass
	{
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type20: "number",
		token.Type24: "upcase",
	},
	// LexSymbol : UnicodeClass ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexZeroOrMore : ∙{ LexAlternates }
	{
		token.Type25: "{",
	},
	// LexZeroOrMore : { ∙LexAlternates }
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexZeroOrMore : { LexAlternates ∙}
	{
		token.Type27: "}",
	},
	// LexZeroOrMore : { LexAlternates } ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// Package : ∙package string_lit
	{
		token.Type21: "package",
	},
	// Package : package ∙string_lit
	{
		token.Type22: "string_lit",
	},
	// Package : package string_lit ∙
	{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// RegExp : ∙LexSymbol
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// RegExp : LexSymbol ∙
	{
		token.Type2:  ")",
		token.Type5:  ";",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type26: "|",
		token.Type27: "}",
	},
	// RegExp : ∙LexSymbol RegExp
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// RegExp : LexSymbol ∙RegExp
	{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// RegExp : LexSymbol RegExp ∙
	{
		token.Type2:  ")",
		token.Type5:  ";",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type26: "|",
		token.Type27: "}",
	},
	// Rule : ∙LexRule
	{
		token.Type23: "tokid",
	},
	// Rule : LexRule ∙
	{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rule : ∙SyntaxRule
	{
		token.Type19: "nt",
	},
	// Rule : SyntaxRule ∙
	{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : ∙Rule
	{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : Rule ∙
	{
		token.EOF: "EOF",
	},
	// Rules : ∙Rule Rules
	{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : Rule ∙Rules
	{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : Rule Rules ∙
	{
		token.EOF: "EOF",
	},
	// SyntaxAlternate : ∙SyntaxSymbols
	{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternate : SyntaxSymbols ∙
	{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxAlternate : ∙empty
	{
		token.Type15: "empty",
	},
	// SyntaxAlternate : empty ∙
	{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxAlternates : ∙SyntaxAlternate
	{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternates : SyntaxAlternate ∙
	{
		token.Type5: ";",
	},
	// SyntaxAlternates : ∙SyntaxAlternate | SyntaxAlternates
	{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternates : SyntaxAlternate ∙| SyntaxAlternates
	{
		token.Type26: "|",
	},
	// SyntaxAlternates : SyntaxAlternate | ∙SyntaxAlternates
	{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternates : SyntaxAlternate | SyntaxAlternates ∙
	{
		token.Type5: ";",
	},
	// SyntaxRule : ∙nt : SyntaxAlternates ;
	{
		token.Type19: "nt",
	},
	// SyntaxRule : nt ∙: SyntaxAlternates ;
	{
		token.Type4: ":",
	},
	// SyntaxRule : nt : ∙SyntaxAlternates ;
	{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxRule : nt : SyntaxAlternates ∙;
	{
		token.Type5: ";",
	},
	// SyntaxRule : nt : SyntaxAlternates ; ∙
	{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// SyntaxSymbol : ∙nt
	{
		token.Type19: "nt",
	},
	// SyntaxSymbol : nt ∙
	{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbol : ∙tokid
	{
		token.Type23: "tokid",
	},
	// SyntaxSymbol : tokid ∙
	{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbol : ∙string_lit
	{
		token.Type22: "string_lit",
	},
	// SyntaxSymbol : string_lit ∙
	{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbols : ∙SyntaxSymbol
	{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxSymbols : SyntaxSymbol ∙
	{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxSymbols : ∙SyntaxSymbol SyntaxSymbols
	{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxSymbols : SyntaxSymbol ∙SyntaxSymbols
	{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxSymbols : SyntaxSymbol SyntaxSymbols ∙
	{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// UnicodeClass : ∙letter
	{
		token.Type16: "letter",
	},
	// UnicodeClass : letter ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// UnicodeClass : ∙upcase
	{
		token.Type24: "upcase",
	},
	// UnicodeClass : upcase ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// UnicodeClass : ∙lowcase
	{
		token.Type17: "lowcase",
	},
	// UnicodeClass : lowcase ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// UnicodeClass : ∙number
	{
		token.Type20: "number",
	},
	// UnicodeClass : number ∙
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
}

var followSets = []map[token.Type]string{
	// GoGLL
	{
		token.EOF: "EOF",
	},
	// LexAlternates
	{
		token.Type2:  ")",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type27: "}",
	},
	// LexBracket
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexGroup
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOneOrMore
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOptional
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexRule
	{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// LexSymbol
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexZeroOrMore
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// Package
	{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// RegExp
	{
		token.Type2:  ")",
		token.Type5:  ";",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type26: "|",
		token.Type27: "}",
	},
	// Rule
	{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules
	{
		token.EOF: "EOF",
	},
	// SyntaxAlternate
	{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxAlternates
	{
		token.Type5: ";",
	},
	// SyntaxRule
	{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// SyntaxSymbol
	{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbols
	{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// UnicodeClass
	{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
}

/*** Errors ***/

/*
Error is returned by Parse at every point at which the parser fails to parse
a grammar production. For non-LL-1 grammars there will be an error for each
alternate attempted by the parser.

The errors are sorted in descending order of input position (index of token in
the stream of tokens).

Normally the error of interest is the one that has parsed the largest number of
tokens.
*/
type Error struct {
	// Index of token that caused the error.
	cI int

	// Grammar slot at which the error occured.
	Slot slot.Label

	// The token at which the error occurred.
	Token *token.Token

	// The line and column in the input text at which the error occurred
	Line, Column int

	// The tokens expected at the point where the error occurred
	Expected map[token.Type]string
}

func (pe *Error) String() string {
	w := new(bytes.Buffer)
	fmt.Fprintf(w, "Parse Error: %s I[%d]=%s at line %d col %d\n",
		pe.Slot, pe.cI, pe.Token, pe.Line, pe.Column)
	exp := []string{}
	for _, e := range pe.Expected {
		exp = append(exp, e)
	}
	fmt.Fprintf(w, "Expected one of: [%s]", strings.Join(exp, ","))
	return w.String()
}

func (p *parser) parseError(slot slot.Label, i int, expected map[token.Type]string) {
	pe := &Error{cI: i, Slot: slot, Token: p.lex.Tokens[i], Expected: expected}
	p.parseErrors = append(p.parseErrors, pe)
}

func (p *parser) sortParseErrors() {
	sort.Slice(p.parseErrors,
		func(i, j int) bool {
			return p.parseErrors[j].Token.Lext() < p.parseErrors[i].Token.Lext()
		})
	for _, pe := range p.parseErrors {
		pe.Line, pe.Column = p.lex.GetLineColumn(pe.Token.Lext())
	}
}
